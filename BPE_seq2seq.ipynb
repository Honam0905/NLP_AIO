{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":349737,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":292065,"modelId":312718}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install evaluate datasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T13:25:07.562190Z","iopub.execute_input":"2025-04-21T13:25:07.562960Z","iopub.status.idle":"2025-04-21T13:25:12.652605Z","shell.execute_reply.started":"2025-04-21T13:25:07.562930Z","shell.execute_reply":"2025-04-21T13:25:12.651810Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nCollecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec, evaluate\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.3.3.83 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.9.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.3.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.8.93 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.8.93 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed evaluate-0.4.3 fsspec-2024.12.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Import library","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport random\nimport numpy as np\nfrom datasets import load_dataset\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import BPE\nfrom tokenizers.trainers import BpeTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\nimport tqdm\nimport evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T13:25:12.654170Z","iopub.execute_input":"2025-04-21T13:25:12.654665Z","iopub.status.idle":"2025-04-21T13:25:33.558042Z","shell.execute_reply.started":"2025-04-21T13:25:12.654611Z","shell.execute_reply":"2025-04-21T13:25:33.557174Z"}},"outputs":[{"name":"stderr","text":"2025-04-21 13:25:17.448567: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745241917.652473      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745241917.711031      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Load dataset","metadata":{}},{"cell_type":"code","source":"ds = load_dataset(\"thainq107/iwslt2015-en-vi\")\ntrain_data, valid_data, test_data = ds[\"train\"], ds[\"validation\"], ds[\"test\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T13:25:33.559003Z","iopub.execute_input":"2025-04-21T13:25:33.559697Z","iopub.status.idle":"2025-04-21T13:25:35.445228Z","shell.execute_reply.started":"2025-04-21T13:25:33.559665Z","shell.execute_reply":"2025-04-21T13:25:35.444411Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/522 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60bd60be8a154094a59934c247a86f72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/17.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8c904ebba6d4afabfc350ec1816cda7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/181k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c77bb46205e4409a9d4904b4c27c1a14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/133317 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51820ed36bd84830b73ba473745c38be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/1268 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a10c6db60bf4f55a4f13cc570216e4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1268 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2260b8678e1844f583cc5271ecbb4646"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"ds ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T13:25:35.446733Z","iopub.execute_input":"2025-04-21T13:25:35.447003Z","iopub.status.idle":"2025-04-21T13:25:35.452386Z","shell.execute_reply.started":"2025-04-21T13:25:35.446985Z","shell.execute_reply":"2025-04-21T13:25:35.451668Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['en', 'vi'],\n        num_rows: 133317\n    })\n    validation: Dataset({\n        features: ['en', 'vi'],\n        num_rows: 1268\n    })\n    test: Dataset({\n        features: ['en', 'vi'],\n        num_rows: 1268\n    })\n})"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"# Train BPE tokenizers","metadata":{}},{"cell_type":"code","source":"# English tokenizer\ntokenizer_en = Tokenizer(BPE())\ntokenizer_en.pre_tokenizer = Whitespace()\ntrainer_en = BpeTrainer(\n    vocab_size=30000,\n    special_tokens=[\"<unk>\", \"<p ad>\", \"<sos>\", \"<eos>\"]\n)\ntokenizer_en.train_from_iterator(train_data[\"en\"], trainer=trainer_en)\n\n# Vietnamese tokenizer\ntokenizer_vi = Tokenizer(BPE())\ntokenizer_vi.pre_tokenizer = Whitespace()\ntrainer_vi = BpeTrainer(\n    vocab_size=30000,\n    special_tokens=[\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"]\n)\ntokenizer_vi.train_from_iterator(train_data[\"vi\"], trainer=trainer_vi)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T13:25:54.348202Z","iopub.execute_input":"2025-04-21T13:25:54.348503Z","iopub.status.idle":"2025-04-21T13:25:59.709849Z","shell.execute_reply.started":"2025-04-21T13:25:54.348474Z","shell.execute_reply":"2025-04-21T13:25:59.708961Z"}},"outputs":[{"name":"stdout","text":"\n\n\n\n\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# Tokenize & numericalize","metadata":{}},{"cell_type":"code","source":"def tokenize_example(example, tokenizer_src, tokenizer_trg, sos_token, eos_token, max_length=1000):\n    # Source = English, Target = Vietnamese\n    src_ids = tokenizer_src.encode(example[\"en\"]).ids[:max_length]\n    trg_ids = tokenizer_trg.encode(example[\"vi\"]).ids[:max_length]\n    # Add <sos> and <eos>\n    src = [tokenizer_src.token_to_id(sos_token)] + src_ids + [tokenizer_src.token_to_id(eos_token)]\n    trg = [tokenizer_trg.token_to_id(sos_token)] + trg_ids + [tokenizer_trg.token_to_id(eos_token)]\n    return {\"en_ids\": src, \"vi_ids\": trg}\n\nfn_kwargs = {\n    \"tokenizer_src\": tokenizer_en,\n    \"tokenizer_trg\": tokenizer_vi,\n    \"sos_token\": \"<sos>\",\n    \"eos_token\": \"<eos>\",\n    \"max_length\": 30\n}\n\n# Note: we no longer remove_columns=[\"en\",\"vi\"]\ntrain_data = train_data.map(tokenize_example, fn_kwargs=fn_kwargs)\nvalid_data = valid_data.map(tokenize_example, fn_kwargs=fn_kwargs)\ntest_data  = test_data.map(tokenize_example, fn_kwargs=fn_kwargs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T13:25:59.711255Z","iopub.execute_input":"2025-04-21T13:25:59.711475Z","iopub.status.idle":"2025-04-21T13:26:29.463325Z","shell.execute_reply.started":"2025-04-21T13:25:59.711459Z","shell.execute_reply":"2025-04-21T13:26:29.462557Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/133317 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c6f1ced97a04d8ab66d1107c51a8503"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1268 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"133b7e13de5d4d16bd5f6ada5a11b52d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1268 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64ab8d708ff04c1ea66292c569e5f0c8"}},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"# Convert lists to torch.Tensor","metadata":{}},{"cell_type":"code","source":"def to_tensor(example):\n    return {\n        \"en_ids\": torch.tensor(example[\"en_ids\"], dtype=torch.long),\n        \"vi_ids\": torch.tensor(example[\"vi_ids\"], dtype=torch.long)\n    }\n\n# after tokenization (lists of ints in en_ids/vi_ids):\ntrain_data = train_data.with_format(\n    type=\"torch\",\n    columns=[\"en_ids\",\"vi_ids\"],\n    output_all_columns=True\n)\nvalid_data = valid_data.with_format(type=\"torch\", columns=[\"en_ids\",\"vi_ids\"], output_all_columns=True)\ntest_data  = test_data.with_format(type=\"torch\", columns=[\"en_ids\",\"vi_ids\"], output_all_columns=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T13:26:29.464105Z","iopub.execute_input":"2025-04-21T13:26:29.464878Z","iopub.status.idle":"2025-04-21T13:26:29.472611Z","shell.execute_reply.started":"2025-04-21T13:26:29.464852Z","shell.execute_reply":"2025-04-21T13:26:29.471895Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# DataLoader setup","metadata":{}},{"cell_type":"code","source":"def get_collate_fn(pad_id):\n    def collate_fn(batch):\n        src = [ex[\"en_ids\"] for ex in batch]\n        trg = [ex[\"vi_ids\"] for ex in batch]\n        src = nn.utils.rnn.pad_sequence(src, padding_value=pad_id)\n        trg = nn.utils.rnn.pad_sequence(trg, padding_value=pad_id)\n        return {\"en_ids\": src, \"vi_ids\": trg}\n    return collate_fn\n\npad_id = tokenizer_en.token_to_id(\"<pad>\")\nbatch_size = 32\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=batch_size,\n    collate_fn=get_collate_fn(pad_id), shuffle=True\n)\nvalid_loader = torch.utils.data.DataLoader(\n    valid_data, batch_size=batch_size,\n    collate_fn=get_collate_fn(pad_id)\n)\ntest_loader  = torch.utils.data.DataLoader(\n    test_data,  batch_size=batch_size,\n    collate_fn=get_collate_fn(pad_id)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T13:26:29.474655Z","iopub.execute_input":"2025-04-21T13:26:29.474961Z","iopub.status.idle":"2025-04-21T13:26:29.753688Z","shell.execute_reply.started":"2025-04-21T13:26:29.474930Z","shell.execute_reply":"2025-04-21T13:26:29.752998Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# Model Seq2Seq","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(input_dim, emb_dim)\n        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, src):\n        # src = [src_len, batch_size]\n        embedded = self.dropout(self.embedding(src))\n        outputs, (hidden, cell) = self.rnn(embedded)\n        return hidden, cell\n\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(output_dim, emb_dim)\n        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n        self.fc_out = nn.Linear(hid_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input, hidden, cell):\n        # input = [batch_size]\n        input = input.unsqueeze(0)\n        # input = [1, batch_size]\n        embedded = self.dropout(self.embedding(input))\n        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n        # output = [1, batch_size, hid_dim]\n        prediction = self.fc_out(output.squeeze(0))\n        # prediction = [batch_size, output_dim]\n        return prediction, hidden, cell\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        assert encoder.rnn.hidden_size == decoder.rnn.hidden_size\n        assert encoder.rnn.num_layers == decoder.rnn.num_layers\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        # src = [src_len, batch_size], trg = [trg_len, batch_size]\n        batch_size = trg.shape[1]\n        trg_len = trg.shape[0]\n        vocab_size = self.decoder.fc_out.out_features\n        outputs = torch.zeros(trg_len, batch_size, vocab_size).to(self.device)\n\n        hidden, cell = self.encoder(src)\n        input = trg[0, :]  # <sos>\n\n        for t in range(1, trg_len):\n            output, hidden, cell = self.decoder(input, hidden, cell)\n            outputs[t] = output\n            teacher_force = random.random() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            input = trg[t] if teacher_force else top1\n\n        return outputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T13:26:29.754544Z","iopub.execute_input":"2025-04-21T13:26:29.754754Z","iopub.status.idle":"2025-04-21T13:26:29.767207Z","shell.execute_reply.started":"2025-04-21T13:26:29.754738Z","shell.execute_reply":"2025-04-21T13:26:29.766452Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Initialize Model + Training Setup","metadata":{}},{"cell_type":"code","source":"import os\n# right:\nINPUT_DIM  = tokenizer_en.get_vocab_size()\nOUTPUT_DIM = tokenizer_vi.get_vocab_size()\nENC_EMB_DIM = 256\nDEC_EMB_DIM = 256\nHID_DIM     = 512\nN_LAYERS    = 2\nENC_DROPOUT = 0.5\nDEC_DROPOUT = 0.5\nDEVICE      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nenc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\ndec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\nmodel = Seq2Seq(enc, dec, DEVICE).to(DEVICE)\n\ndef init_weights(m):\n    for p in m.parameters():\n        nn.init.uniform_(p.data, -0.08, 0.08)\nmodel.apply(init_weights)\n\noptimizer = optim.Adam(model.parameters())\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n\nCHECKPOINT_PATH = '/kaggle/input/bpe_seq2seq/pytorch/default/1/best-model.pt'  \n\nif os.path.isfile(CHECKPOINT_PATH):\n    model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=DEVICE))\n    print(f\"✔ Loaded checkpoint from {CHECKPOINT_PATH}, resuming training.\")\nelse:\n    # only initialize weights if there's no checkpoint\n    def init_weights(m):\n        for p in m.parameters():\n            nn.init.uniform_(p.data, -0.08, 0.08)\n    model.apply(init_weights)\n    print(\"✗ No checkpoint found — training from scratch.\")\n\nprint(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T13:57:29.104236Z","iopub.execute_input":"2025-04-21T13:57:29.104504Z","iopub.status.idle":"2025-04-21T13:57:29.692149Z","shell.execute_reply.started":"2025-04-21T13:57:29.104475Z","shell.execute_reply":"2025-04-21T13:57:29.691281Z"}},"outputs":[{"name":"stdout","text":"✔ Loaded checkpoint from /kaggle/input/bpe_seq2seq/pytorch/default/1/best-model.pt, resuming training.\nTrainable parameters: 38,106,416\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/884584735.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=DEVICE))\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"# Training and Evaluation Functions","metadata":{}},{"cell_type":"code","source":"def train_fn(model, loader, optimizer, criterion, clip, device):\n    model.train()\n    epoch_loss = 0\n    for batch in loader:\n        src = batch[\"en_ids\"].to(device)\n        trg = batch[\"vi_ids\"].to(device)\n        optimizer.zero_grad()\n        output = model(src, trg, teacher_forcing_ratio=0.5)\n        # output = [trg_len, batch_size, vocab_size]\n        output_dim = output.shape[-1]\n        out = output[1:].view(-1, output_dim)\n        tgt = trg[1:].view(-1)\n        loss = criterion(out, tgt)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        epoch_loss += loss.item()\n    return epoch_loss / len(loader)\n\ndef eval_fn(model, loader, criterion, device):\n    model.eval()\n    epoch_loss = 0\n    with torch.no_grad():\n        for batch in loader:\n            src = batch[\"en_ids\"].to(device)\n            trg = batch[\"vi_ids\"].to(device)\n            output = model(src, trg, teacher_forcing_ratio=0.5)  # no teacher forcing\n            output_dim = output.shape[-1]\n            out = output[1:].view(-1, output_dim)\n            tgt = trg[1:].view(-1)\n            loss = criterion(out, tgt)\n            epoch_loss += loss.item()\n    return epoch_loss / len(loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T13:57:34.925411Z","iopub.execute_input":"2025-04-21T13:57:34.925880Z","iopub.status.idle":"2025-04-21T13:57:34.933096Z","shell.execute_reply.started":"2025-04-21T13:57:34.925856Z","shell.execute_reply":"2025-04-21T13:57:34.932248Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"# Training loops","metadata":{}},{"cell_type":"code","source":"N_EPOCHS = 2\nCLIP     = 1.0\n\nbest_valid = float('inf')\nfor epoch in range(N_EPOCHS):\n    train_loss = train_fn(model, train_loader, optimizer, criterion, CLIP, DEVICE)\n    valid_loss = eval_fn(model, valid_loader, criterion, DEVICE)\n    if valid_loss < best_valid:\n        best_valid = valid_loss\n        torch.save(model.state_dict(), 'best-model.pt')\n    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.3f} | Train PPL: {np.exp(train_loss):.3f}\")\n    print(f\"          | Val   Loss: {valid_loss:.3f} | Val   PPL: {np.exp(valid_loss):.3f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T13:57:36.821044Z","iopub.execute_input":"2025-04-21T13:57:36.821334Z","iopub.status.idle":"2025-04-21T14:32:14.452481Z","shell.execute_reply.started":"2025-04-21T13:57:36.821313Z","shell.execute_reply":"2025-04-21T14:32:14.451877Z"}},"outputs":[{"name":"stdout","text":"Epoch 1 | Train Loss: 3.273 | Train PPL: 26.382\n          | Val   Loss: 4.087 | Val   PPL: 59.551\nEpoch 2 | Train Loss: 3.245 | Train PPL: 25.669\n          | Val   Loss: 4.108 | Val   PPL: 60.810\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"# Testing + BLEU","metadata":{}},{"cell_type":"code","source":"model.load_state_dict(torch.load('best-model.pt'))\ntest_loss = eval_fn(model, test_loader, criterion, DEVICE)\nprint(f\"Test Loss: {test_loss:.3f} | Test PPL: {np.exp(test_loss):.3f}\")\n\n# Translation helper\ndef translate_sentence(\n    sentence, model, tokenizer_src, tokenizer_trg,\n    lower=True, sos_token=\"<sos>\", eos_token=\"<eos>\",\n    device=DEVICE, max_len=30\n):\n    model.eval()\n    tokens = sentence.split()  # already whitespace-tokenized\n    tokens = [sos_token] + tokens + [eos_token]\n    src_ids = tokenizer_src.encode(\" \".join(tokens)).ids\n    src_tensor = torch.LongTensor(src_ids).unsqueeze(1).to(device)\n    hidden, cell = model.encoder(src_tensor)\n\n    outputs = [tokenizer_trg.token_to_id(sos_token)]\n    for _ in range(max_len):\n        prev = torch.LongTensor([outputs[-1]]).to(device)\n        pred, hidden, cell = model.decoder(prev, hidden, cell)\n        top1 = pred.argmax(1).item()\n        outputs.append(top1)\n        if top1 == tokenizer_trg.token_to_id(eos_token):\n            break\n\n    return tokenizer_trg.decode(outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T14:32:14.453808Z","iopub.execute_input":"2025-04-21T14:32:14.454012Z","iopub.status.idle":"2025-04-21T14:32:19.345718Z","shell.execute_reply.started":"2025-04-21T14:32:14.453996Z","shell.execute_reply":"2025-04-21T14:32:19.345019Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/717956662.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best-model.pt'))\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 4.036 | Test PPL: 56.574\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"# Compute BLEU on test set\nbleu = evaluate.load(\"bleu\")\npredictions = []\nreferences  = []\n\nfor ex in tqdm.tqdm(test_data):\n    # ex[\"en\"] & ex[\"vi\"] are still present because we didn't remove them\n    pred = translate_sentence(\n        ex[\"en\"], model,\n        tokenizer_en, tokenizer_vi,\n        device=DEVICE\n    )\n    predictions.append(pred)\n    references.append([ex[\"vi\"]])\n\nresults = bleu.compute(predictions=predictions, references=references)\nprint(f\"BLEU score = {results['bleu']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T14:32:19.346266Z","iopub.execute_input":"2025-04-21T14:32:19.346493Z","iopub.status.idle":"2025-04-21T14:32:41.164797Z","shell.execute_reply.started":"2025-04-21T14:32:19.346475Z","shell.execute_reply":"2025-04-21T14:32:41.164048Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0275f3ec6b2d44d6b8f78100fcdf6d46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ab8c531bc0c498b97b3f4ef05f046b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e791b78b0bf42818bec003ea05b2025"}},"metadata":{}},{"name":"stderr","text":"100%|██████████| 1268/1268 [00:20<00:00, 61.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"BLEU score = 0.0685\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"# test model","metadata":{}},{"cell_type":"code","source":"sentence = test_data[0][\"en\"]\nexpected_translation = test_data[0][\"vi\"]\nprint(\"Source (English):\", sentence)\nprint(\"Expected Translation (Vietnamese):\", expected_translation)\ntranslation = translate_sentence(sentence, model, tokenizer_en, tokenizer_vi,device=DEVICE)\nprint(\"Model Translation:\", translation)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T14:32:41.166166Z","iopub.execute_input":"2025-04-21T14:32:41.166437Z","iopub.status.idle":"2025-04-21T14:32:41.197882Z","shell.execute_reply.started":"2025-04-21T14:32:41.166418Z","shell.execute_reply":"2025-04-21T14:32:41.197251Z"}},"outputs":[{"name":"stdout","text":"Source (English): When I was little , I thought my country was the best on the planet , and I grew up singing a song called &quot; Nothing To Envy . &quot;\nExpected Translation (Vietnamese): Khi tôi còn nhỏ , Tôi nghĩ rằng BắcTriều Tiên là đất nước tốt nhất trên thế giới và tôi thường hát bài &quot; Chúng ta chẳng có gì phải ghen tị . &quot;\nModel Translation: Khi tôi nhỏ , tôi nghĩ rằng tôi là đất nước là một thế giới tốt đẹp nhất và tôi đã hát rằng tôi gọi là & quot ; Ain\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"import torch\n\n# Check if CUDA is available\nif torch.cuda.is_available():\n    # Print total GPU memory\n    total_memory = torch.cuda.get_device_properties(0).total_memory\n    # Print reserved memory\n    reserved_memory = torch.cuda.memory_reserved(0)\n    # Print allocated memory\n    allocated_memory = torch.cuda.memory_allocated(0)\n    # Calculate and print free memory\n    free_memory = total_memory - reserved_memory\n    \n    print(f\"Total GPU memory: {total_memory / 1e9:.2f} GB\")\n    print(f\"Reserved memory: {reserved_memory / 1e9:.2f} GB\")\n    print(f\"Allocated memory: {allocated_memory / 1e9:.2f} GB\")\n    print(f\"Free memory: {free_memory / 1e9:.2f} GB\")\nelse:\n    print(\"CUDA is not available\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Free cached memory\ntorch.cuda.empty_cache()\n\n# Delete unused variables\nimport gc\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}